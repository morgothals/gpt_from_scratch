## üß© Az LLM tan√≠t√°sa: hogyan lesz a tokenb≈ël tud√°s?

Miut√°n m√°r meg√©rtett√ºk, **mi az a tokeniz√°ci√≥**, √©s hogyan lehet egy sz√∂veget **karakterekre bontani**, majd sz√°mokk√° alak√≠tani (tokenek), √©rdemes meg√©rteni, **hogyan tanul egy nyelvi modell** ezekb≈ël a tokenekb≈ël.

A tokeniz√°l√°s √∂nmag√°ban m√©g nem el√©g. Az csak azt oldja meg, hogy a nyelvet **√°talak√≠tjuk a g√©p sz√°m√°ra feldolgozhat√≥ form√°ba**. De ett≈ël m√©g nem fogja *√©rteni* vagy *j√≥solni*, mi a k√∂vetkez≈ë sz√≥ egy mondatban, vagy hogy mit v√°laszoljon egy k√©rd√©sre.

Ehhez sz√ºks√©g van a **tanul√°sra**, vagyis arra, hogy a modell k√©pes legyen **√∂sszef√ºgg√©seket felismerni a tokenek k√∂z√∂tt**. Ezt a folyamatot mutatja be az al√°bbi k√≠s√©rlet.

---

## üéì Mi√©rt ezzel kezd√ºnk?

A nagy nyelvi modellek (LLM-ek) belsej√©ben t√∂bb milli√≥ (vagy milli√°rd) param√©ter tal√°lhat√≥, amelyek finoman szab√°lyozz√°k, mit "gondol" a modell egy adott sz√∂vegdarabr√≥l. De hogy meg√©rts√ºk ezt a sokas√°got, el≈ëbb meg kell tanulnunk **hogyan tanul egyetlen s√∫ly** ‚Äî hiszen ez a legalapvet≈ëbb √©p√≠t≈ëeleme minden neur√°lis modellnek.

Ez a k√≠s√©rlet egyetlen bemenetet, egyetlen s√∫lyt √©s egy c√©l√©rt√©ket haszn√°l, hogy megmutassa:

* Hogyan hoz l√©tre egy modell egy kimenetet?
* Hogyan m√©ri meg, hogy ez j√≥-e?
* Hogyan m√≥dos√≠tja a saj√°t viselked√©s√©t ennek alapj√°n?

---

## üîß Fogalmak, amiket itt bevezet√ºnk

### 1. **S√∫ly (weight)** ‚Äì ez az, amit a modell ‚Äûmegtanul‚Äù

Egy sz√°m, ami meghat√°rozza, hogyan hat a bemenet a kimenetre. A tanul√°s sor√°n ezt fogjuk finomhangolni.

### 2. **El≈ërefel√© sz√°mol√°s (forward pass)** ‚Äì kisz√°moljuk a jelenlegi kimenetet

P√©ld√°ul: ha a bemenet 3, √©s a s√∫ly 2, akkor `kimenet = 3 √ó 2 = 6`.

### 3. **Loss (hiba, vesztes√©g)** ‚Äì mennyire volt rossz a becsl√©s?

A c√©l az, hogy a `kimenet` k√∂zel legyen a `c√©l√©rt√©khez` (pl. 12). A loss azt m√©ri, mekkora a k√ºl√∂nbs√©g:

$$
\text{loss} = (kimenet - c√©l)^2
$$

### 4. **Visszaterjeszt√©s (backpropagation)** ‚Äì hogyan m√≥dos√≠tsuk a s√∫lyt?

A gradiens megmutatja: merre √©s milyen m√©rt√©kben kell elmozd√≠tani a s√∫lyt, hogy kisebb legyen a hiba.

### 5. **Gradient Descent** ‚Äì a s√∫ly t√©nyleges friss√≠t√©se

A gradiens ir√°ny√°ban elmozd√≠tjuk a s√∫lyt, √©s √∫jra sz√°molunk. Ezt a folyamatot ism√©telj√ºk, am√≠g a loss el√©g kicsi nem lesz.

---

## üìà Hogyan kapcsol√≥dik ez az LLM-ekhez?

Egy LLM belsej√©ben:

* Minden tokenhez egy vektor (embedding) tartozik
* Ezeket a vektorokat **s√∫lym√°trixok** seg√≠ts√©g√©vel m√≥dos√≠tjuk
* A s√∫lyokat √∫gy tan√≠tjuk, hogy **j√≥solni tudja a k√∂vetkez≈ë tokent**
* A tanul√°s alapja: pontosan **ugyanez a logika**, mint ebben az egyv√°ltoz√≥s p√©ld√°ban!

A k√ºl√∂nbs√©g csak annyi, hogy ott:

* T√∂bb bemenet √©s s√∫ly van
* A kimenet vektor form√°j√∫ (pl. val√≥sz√≠n≈±s√©gi eloszl√°s a sz√≥k√©szlet felett)
* T√∂bb r√©tegen (transformeren) kereszt√ºl t√∂rt√©nik a sz√°m√≠t√°s

De **a matematikai alapelvek ugyanazok**: el≈ërefel√© sz√°mol√°s, loss, visszaterjeszt√©s, s√∫lyfriss√≠t√©s.

---


