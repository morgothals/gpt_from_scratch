

## üß† √öj fogalmak a 3. k√≠s√©rletben ‚Äì Bigram modell √©s a Cross-Entropy

Az el≈ëz≈ë k√≠s√©rletekben megtanultuk:

* hogyan tanul egyetlen s√∫ly,
* hogyan haszn√°ljuk a PyTorch-ot gradienssz√°m√≠t√°shoz,
* hogyan lehet egy kimenetet kisz√°m√≠tani √©s visszaterjeszteni.

Ebben a harmadik p√©ld√°ban m√°r egy **egyszer≈± nyelvi modell** form√°l√≥dik. N√©zz√ºk meg, **milyen √∫j fogalmak jelentek meg, √©s mi√©rt fontosak.**

---

### üîó **Bigram modell** ‚Äì Az els≈ë l√©p√©s a nyelvi meg√©rt√©s fel√©

A **bigram modell** azt jelenti, hogy:

> *Egy adott karakter alapj√°n megpr√≥b√°ljuk megj√≥solni, mi j√∂n ut√°na.*

Ez a legegyszer≈±bb nyelvi modell:

* Nem n√©z vissza kor√°bbi szavakra, csak az **aktu√°lis karaktert** haszn√°lja kontextusnak.
* Pl. `"l"` ut√°n `"o"` j√∂n ‚Üí ezt meg szeretn√©nk tan√≠tani a modellnek.

Ez **l√©nyegesen k√ºl√∂nb√∂zik** az eddigi p√©ld√°kt√≥l, ahol:

* Egyetlen bemenet volt (`x`), √©s egy c√©l√©rt√©k (`target`)
* Most viszont egy **teljes sz√∂vegszekvencia** ment√©n tan√≠tunk, ahol:

  * `xs` = az aktu√°lis karakterek
  * `ys` = a k√∂vetkez≈ë karakterek (c√©lok)

---

### üì¶ **S√∫lym√°trix fel√©p√≠t√©se** ‚Äì Mi√©rt `torch.randn((vocab_size, vocab_size))`?

A k√∂vetkez≈ë sor:

```python
W = torch.randn((vocab_size, vocab_size), requires_grad=True)
```

jelenti a modell s√∫lyait. De **mi√©rt pont √≠gy**?

#### üîπ `vocab_size √ó vocab_size` jelent√©se

* A **sorok** megfelelnek a bemeneti tokeneknek (pl. `"h"`, `"e"`, `"l"`, stb.)
* Az **oszlopok** a kimeneti tokenekre adott ‚Äûv√°laszok‚Äù (j√≥slatok)
* Vagyis: minden sor egyfajta **‚Äûj√≥sl√≥ vektor‚Äù**, ami megmondja, milyen karakter k√∂vetkezhet

**P√©lda:**
Ha `vocab_size = 5`, akkor:

```plaintext
W =
[ [ 0.1, 0.3, -0.7, ... ],  ‚Üê 'a' ut√°n mi j√∂het?
  [ 0.5, -0.2, 0.9, ... ],  ‚Üê 'b' ut√°n mi j√∂het?
  ...
]
```

> Teh√°t: `W[token_id]` = logitok az √∂sszes lehets√©ges kimenetre

#### üîπ `torch.randn(...)` ‚Äì Mi ez?

Ez a f√ºggv√©ny **v√©letlenszer≈± sz√°mokat gener√°l**, norm√°lis eloszl√°sb√≥l:

* √Åtlag: 0
* Sz√≥r√°s: 1
* C√©l: *a tan√≠t√°s indul√°sakor ne legyen minden null√°s vagy konstans*, mert az akad√°lyozza a tanul√°st

---

### üí° **Logitok** ‚Äì A softmax el≈ëtti nyers √©rt√©kek

A `W[xs]` visszaadja a ‚Äûlogitokat‚Äù:

```python
logits = W[xs]  # shape: (T, vocab_size)
```

A logit:

* M√©g **nem val√≥sz√≠n≈±s√©g**, hanem csak egy ‚Äûsk√°la‚Äù minden kimeneti tokenhez.
* Min√©l nagyobb egy logit, ann√°l val√≥sz√≠n≈±bbnek ‚Äûgondolja‚Äù a modell az adott karaktert.
* A softmax fogja **val√≥sz√≠n≈±s√©gg√© alak√≠tani** (ezt a `cross_entropy` automatikusan elv√©gzi).


---

## üìê √ñsszefoglalva: hogyan olvassuk a s√∫lym√°trixot (`W`)?

### üß± A `W` m√°trix egy `vocab_size √ó vocab_size` m√©ret≈± m√°trix, ahol

* **Minden sor** megfelel egy *bemeneti tokennek* (egy karakternek),
* **Minden oszlop** megfelel egy *lehets√©ges kimeneti tokennek* (a k√∂vetkez≈ë karakternek),
* **Az √©rt√©kek (logitok)** a sor‚Äìoszlop metszetben azt mondj√°k meg, *mekkora ‚Äûhajland√≥s√°ga‚Äù van a modellnek* arra, hogy az adott karakter ut√°n egy m√°sik karakter k√∂vetkezzen.

---

### üîé P√©lda: `'h'` karakter tokenje = 3 ‚Üí 3. sor

* `W[3]` az a sor, amely megmondja: **ha a bemeneti karakter `'h'`, akkor**:

  * `W[3][2]` ‚Üí mennyire val√≥sz√≠n≈±, hogy `'e'` j√∂n ut√°na,
  * `W[3][4]` ‚Üí mennyire val√≥sz√≠n≈±, hogy `'l'` j√∂n ut√°na,
  * `W[3][3]` ‚Üí mennyire val√≥sz√≠n≈±, hogy **megint `'h'` j√∂n ut√°na** (teh√°t *√∂nmag√°t j√≥solja*).

> üìå A **3. sor** teh√°t `'h'`-hez tartozik, a **3. oszlop** pedig `'h'` mint k√∂vetkez≈ë karakter.

---

### üîÅ Ez mit jelent?

* Ha pl. `W[3][2] = 1.2` √©s `W[3][3] = -0.4`, akkor:

  * a modell azt ‚Äûhiszi‚Äù, hogy `'h'` ut√°n **val√≥sz√≠n≈±bb** az `'e'`,
  * √©s **kev√©sb√© val√≥sz√≠n≈±**, hogy `'h'` ut√°n √∫jra `'h'` j√∂n.

Ezeket az √©rt√©keket a softmax k√©s≈ëbb **val√≥sz√≠n≈±s√©gi eloszl√°ss√°** alak√≠tja.

---

### üéØ Fontos megjegyz√©s

A `W` sorai teh√°t:

* egy **val√≥sz√≠n≈±s√©gi eloszl√°s logit-alapj√°t** adj√°k,
* √©s minden **egyes karakter ut√°n** k√ºl√∂n ‚Äûj√≥sl√°si profilt‚Äù adnak meg,
* amiket **tan√≠t√°s sor√°n a loss alapj√°n finom√≠tunk**.

---





### üéØ **Cross-Entropy loss** ‚Äì mi√©rt nem n√©gyzetes hib√°t haszn√°lunk?

Az el≈ëz≈ë p√©ld√°kban **n√©gyzetes hiba** volt:

$$
\text{loss} = (j√≥slat - c√©l)^2
$$

Ez akkor j√≥, ha **sz√°mokat** j√≥solunk.

De itt **kateg√≥ri√°kat** j√≥solunk (karaktereket), nem sz√°m√©rt√©keket.
Ez√©rt haszn√°lunk:

```python
loss = F.cross_entropy(logits, ys)
```

Ez azt jelenti:

* A `logits` egy val√≥sz√≠n≈±s√©gi eloszl√°st hat√°roz meg (a softmax √°ltal),
* A `ys` egy kateg√≥ria (pl. az `"o"` karakter indexe),
* A loss kisz√°molja: **mennyi volt az elt√©r√©s az igazi karaktert≈ël**.

Ez a **standard megk√∂zel√≠t√©s oszt√°lyoz√°si probl√©m√°kra**:

* Pl. k√©pfelismer√©s (`macska`, `kutya`), vagy itt: karakter-j√≥sl√°s.

---




Term√©szetesen! N√©zz√ºk meg l√©p√©sr≈ël l√©p√©sre **konkr√©t p√©ld√°n kereszt√ºl**, hogyan √©p√ºl fel √©s v√°ltozik a s√∫lym√°trix (`W`) ebben a **bigram karakterj√≥sl√≥ k√≠s√©rletben**, √©s **milyen konkr√©t sz√°mok** jelennek meg a tensorokban.

---

## üìö Kiindul√°s: a `text = "hello world"` p√©lda

Ez a sz√∂veg alapj√°n az egyedi karakterek:

```python
chars = sorted(list(set("hello world")))
# [' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']
```

Ez√©rt:

* `vocab_size = 8`
* Minden karakterhez egy sorsz√°m (`stoi`), pl.:

  ```python
  {' ': 0, 'd': 1, 'e': 2, 'h': 3, 'l': 4, 'o': 5, 'r': 6, 'w': 7}
  ```

---

## üèó A s√∫lym√°trix (`W`) fel√©p√≠t√©se

```python
W = torch.randn((vocab_size, vocab_size), requires_grad=True)
```

Ez egy 8√ó8-as m√°trix, p√©ld√°ul √≠gy (els≈ë 3 sor kerek√≠tve):

```
W =
[
  [ 0.43, -0.98,  0.22,  1.10,  0.19, -0.17,  0.31, -0.66],  # karakter: ' '
  [ 0.05, -0.34, -0.65,  0.09,  0.55,  0.13,  0.40, -0.27],  # karakter: 'd'
  [-0.23,  0.48, -0.14, -1.11,  0.20,  0.33,  0.11,  0.92],  # karakter: 'e'
  ...
]
```

**√ârtelmez√©s:**

* Pl. az els≈ë sor a `' '` (sz√≥k√∂z) karaktert jel√∂li ‚Üí ez a sor 8 √©rt√©ket tartalmaz: mindegyik azt mutatja, *mennyire val√≥sz√≠n≈±*, hogy az adott karakter ut√°n egy m√°sik karakter j√∂n.

---

## üìå P√©lda tokeniz√°l√°sra: `"hello worl"` ‚Üí `"ello world"`

```python
xs = [3, 2, 4, 4, 5, 0, 7, 5, 6, 4]  # 'h','e','l','l','o',' ','w','o','r','l'
ys = [2, 4, 4, 5, 0, 7, 5, 6, 4, 1]  # 'e','l','l','o',' ','w','o','r','l','d'
```

Teh√°t `xs` a bemenet (input tokenek), `ys` a c√©l (target tokenek).

---

## üßÆ Mit csin√°l a `logits = W[xs]`?

Ez a sor:

* **kiv√°lasztja** a `W` m√°trix `xs` √°ltal megadott sorait.

Ha `xs = [3, 2, 4]`, akkor:

```python
logits = W[[3, 2, 4]]  # shape: (3, 8)
```

Ez a 3 soros m√°trix azt mutatja, hogy:

* `'h'` ut√°n milyen karaktereket j√≥solunk
* `'e'` ut√°n milyen karaktereket j√≥solunk
* `'l'` ut√°n milyen karaktereket j√≥solunk

---

## üéØ Hogyan tanul a s√∫lym√°trix?

Vegy√ºk p√©ld√°nak `"h"` ‚Üí `"e"` √°tmenetet:

* `'h'` tokenje: `3`
* `'e'` tokenje: `2`

A `W[3]` sor az, amit m√≥dos√≠tani akarunk √∫gy, hogy:

* az √©rt√©k a 2-es index≈± oszlopban (azaz az `'e'` logitja) **n√∂vekedjen**,
* a t√∂bbi logit cs√∂kkenjen (vagy stagn√°ljon).

### ‚ú® P√©lda: el≈ëtte

```plaintext
W[3] = [ 0.2, -0.1, 0.05, 0.3, 0.4, -0.2, 0.0, 0.1 ]
                    ‚Üë
                 'e' (index 2)
```

**De a c√©l az, hogy `'h'` ut√°n `'e'` j√∂jj√∂n ‚Üí n√∂velni kell a 2-es √©rt√©ket!**

---

### ‚õè A `loss = F.cross_entropy(logits, ys)` hogyan seg√≠t?

Ez kisz√°molja:

* A `logits` √©rt√©keib≈ël a softmax eloszl√°st
* A c√©loszt√°ly (`ys`) alapj√°n mekkora a hiba

Majd:

```python
loss.backward()
```

Ez:

* kisz√°molja a gradiens √©rt√©k√©t: pl.

  ```python
  W.grad[3] = [  0.01, -0.02, -0.35, 0.04, 0.05, 0.03, 0.01, 0.02 ]
  ```

Mivel a c√©l az volt, hogy a `2. index` (az `'e'` logit) nagyobb legyen, a gradiens ott **negat√≠v** ‚Üí a friss√≠t√©skor ez pozit√≠v ir√°ny√∫ elmozdul√°st eredm√©nyez.

---

### üîÅ Friss√≠t√©s ut√°n (`W -= lr * W.grad`)

```python
W[3][2] = 0.05 - 0.1 * (-0.35) = 0.05 + 0.035 = 0.085
```

**A `'h'` ut√°ni `'e'` logitja megn≈ëtt** ‚Üí a modell er≈ësebben ‚Äûhisz benne‚Äù, hogy `'h'` ut√°n `'e'` j√∂n.

---

## üìà Loss g√∂rbe hat√°sa

Minden ilyen m√≥dos√≠t√°s a `W` m√°trixon:

* k√∂zelebb visz ahhoz, hogy a modell a j√≥ karaktert j√≥solja,
* √©s ez l√°tszik a `loss_history` g√∂rb√©j√©n is: fokozatos cs√∂kken√©s

---

## üß† √ñsszefoglal√°s konkr√©t p√©ld√°val

| L√©p√©s          | Konkr√©t √©rt√©k      | Jelent√©s                                   |
| -------------- | ------------------ | ------------------------------------------ |
| `xs[0] = 3`    | `'h'` tokenje      | Bemeneti karakter                          |
| `ys[0] = 2`    | `'e'` tokenje      | C√©l karakter                               |
| `W[3][2]`      | 0.05 ‚Üí 0.085       | `'h'` ut√°ni `'e'` val√≥sz√≠n≈±s√©ge n≈ëtt       |
| `W.grad[3][2]` | -0.35              | A hibaf√ºggv√©ny szerint ennyit k√©ne n√∂velni |
| Friss√≠t√©s ut√°n | `W[3][2] += 0.035` | A modell tanult a p√©ld√°b√≥l                 |

---






## üß© 3. k√≠s√©rlet ‚Äì A legegyszer≈±bb nyelvi modell: bigram predikci√≥ tan√≠t√°sa

Ez a p√©ldaprogram egy *karakter-alap√∫ bigram nyelvi modellt* tan√≠t meg: megtanulja, hogy **egy adott karakter ut√°n melyik karakter szokott k√∂vetkezni** a megadott sz√∂veg alapj√°n.

P√©ld√°ul:

* A `"h"` ut√°n `"e"` szokott j√∂nni,
* Az `"o"` ut√°n `" "` (sz√≥k√∂z),
* stb.

---

## üÜï Milyen √∫j elemeket vezet be a k√≠s√©rlet?

| √öjdons√°g                      | Mit jelent / mi√©rt fontos?                                                               |
| ----------------------------- | ---------------------------------------------------------------------------------------- |
| **Bigram modell**             | A modell csak egy el≈ëz≈ë karakter alapj√°n pr√≥b√°lja megj√≥solni a k√∂vetkez≈ët                |
| **S√∫lym√°trix = ‚Äûj√≥sl√≥t√°bla‚Äù** | Minden sorban azt tanuljuk meg, hogy az adott karakter ut√°n mi val√≥sz√≠n≈±                 |
| **Cross-entropy loss**        | A kateg√≥ria alap√∫ hibam√©r√©s szabv√°nyos m√≥dja, amikor oszt√°lyokat j√≥solunk (nem sz√°mokat) |
| **Loss g√∂rbe kirajzol√°sa**    | Vizualiz√°ljuk, hogy hogyan cs√∂kken a hiba a tanul√°s sor√°n                                |
| **Token-szekvencia kezel√©s**  | T√∂bb tokenen (karakteren) tanulunk egyszerre                                             |

---

## üß† Mit tan√≠t meg ez a modell?

Ez az els≈ë olyan p√©lda, ahol a modell **nem egyetlen bemenet‚Äìkimenet p√°rb√≥l tanul**, hanem **egy teljes szekvenci√°b√≥l**:

* Bemenet: `"h", "e", "l", "l", "o", ...`
* C√©l: `"e", "l", "l", "o", " ", ...`

√çgy minden l√©p√©sn√©l a modell megtanulja, **hogyan kell a jelenlegi karakterb≈ël k√∂vetkeztetni a k√∂vetkez≈ëre**.

---

## üèó Hogyan √©p√ºl fel a modell?

1. **Tokeniz√°l√°s**

   * Az √∂sszes karakterhez sz√°mot rendel√ºnk (tokenek).
   * `xs`: az √∂sszes bemeneti karakter tokenje.
   * `ys`: az √∂sszes k√∂vetkez≈ë karakter tokenje (targetek).

2. **S√∫lym√°trix `W`**

   * M√©rete: `(vocab_size, vocab_size)`
   * Minden sor azt mondja meg: *ha ez a karakter a bemenet, mi legyen a kimenet*.
   * Tan√≠that√≥ tensor (`requires_grad=True`).

3. **El≈ërefel√© sz√°mol√°s**

   * A `W[xs]` minden tokenhez kiv√°lasztja a hozz√° tartoz√≥ sorokat: ezek a predikci√≥k (logitok).
   * A `cross_entropy` loss m√©ri, mennyire t√©rnek el a j√≥slatok a val√≥s k√∂vetkez≈ë karakterekt≈ël.

4. **Visszaterjeszt√©s √©s friss√≠t√©s**

   * A `.backward()` kisz√°m√≠tja a gradienset.
   * A s√∫lyokat friss√≠tj√ºk: `W -= lr * W.grad`.
   * A gr√°fot nem √©p√≠tj√ºk √∫jra (`with torch.no_grad()`).
   * A gradiens √©rt√©ket lenull√°zzuk.

5. **Vizualiz√°ci√≥**

   * A `loss_history` alapj√°n l√°tjuk, hogyan tanul a modell.

---

## üìà Mit l√°tunk a loss g√∂rb√©n?

A tanul√°s sor√°n a loss √©rt√©ke fokozatosan cs√∂kken, mert:

* A modell megtanulja, hogy pl. `"h"` ut√°n `"e"` j√∂n,
* √âs nem tal√°lgat v√©letlenszer≈±en.

Ez a g√∂rbe **visszajelz√©s a tanul√°s hat√©konys√°g√°r√≥l**.









Ez a k√©t sor a **bigram nyelvi modell** sz√≠v√©t jelenti:

```python
logits = W[xs]                
loss = F.cross_entropy(logits, ys)
```



Ebben a r√©szben pontosan megmutatom **elm√©letileg √©s konkr√©t p√©ld√°n**, hogy *mi t√∂rt√©nik itt, mit tartalmaz a `logits`, √©s hogyan sz√°mol a `cross_entropy`*.

---

## üß± Kontextus ‚Äì el≈ëk√©sz√≠t√©s

Kiindul√≥ adatok:

* `text = "hello world"`
* Karakterk√©szlet (`chars`) = \[' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']
* Tokeniz√°l√°s: `stoi = {' ': 0, 'd': 1, ..., 'h': 3, 'l': 4, 'o': 5, ...}`

Sz√∂veg ‚Üí tokenek (r√©szlet):

```python
xs = [3, 2, 4]  # pl. 'h', 'e', 'l'
ys = [2, 4, 4]  #     'e', 'l', 'l'
```

Teh√°t p√©ld√°ul:

* `'h'` ut√°n `'e'`
* `'e'` ut√°n `'l'`
* `'l'` ut√°n `'l'`

---

## üîç Els≈ë sor: `logits = W[xs]`

### üìå Mit csin√°l?

Ez kiv√°lasztja a `W` m√°trix **xs √°ltal megadott sorait**, teh√°t:

```python
logits = torch.stack([W[3], W[2], W[4]])
```

Ez egy `T √ó vocab_size` alak√∫ tensor:

* `T = 3` (h√°rom bemenet)
* `vocab_size = 8` (ennyi kimeneti token van)

P√©ld√°ul (fikt√≠v logit √©rt√©kekkel):

```plaintext
logits =
[[ 0.1, -0.2,  0.9, 0.0, -0.5, 0.3, 0.1, 0.2],   # W[3] ‚Üí 'h'
 [-0.1,  0.0,  0.2, 0.3,  1.0, 0.0, 0.1, 0.1],   # W[2] ‚Üí 'e'
 [ 0.2, -0.3,  0.1, 0.2,  0.8, 0.0, 0.0, 0.0]]   # W[4] ‚Üí 'l'
```

### üîé √ârtelmez√©s

* Soronk√©nt: egy **j√≥slat-vektor** (logitok) minden lehets√©ges kimeneti tokenre.
* Minden sor azt jelenti: *ha az adott karakter a bemenet, akkor mi a modell becsl√©se a k√∂vetkez≈ë karakter eloszl√°s√°ra.*

---

## üéØ M√°sodik sor: `loss = F.cross_entropy(logits, ys)`

Ez a **cross-entropy loss f√ºggv√©ny**, ami:

1. **Softmax**-ot sz√°mol minden sorra: √°talak√≠tja a logitokat **val√≥sz√≠n≈±s√©gekk√©** (0 √©s 1 k√∂z√∂tt, √∂sszege 1).
2. **Kiv√°lasztja a c√©loszt√°lyhoz tartoz√≥ val√≥sz√≠n≈±s√©get** (amit `ys` mutat).
3. **Negat√≠v logaritmust vesz**:

   $$
   \text{loss}_i = -\log(p_{\text{c√©l}})
   $$
4. Az √∂sszes sorra ezt kisz√°molja, majd √°tlagolja.

---

## üîç Konkr√©t p√©lda: k√©zzel kisz√°molva

N√©zz√ºk meg a fenti `logits` els≈ë sor√°t:

```plaintext
logits[0] = [ 0.1, -0.2, 0.9, 0.0, -0.5, 0.3, 0.1, 0.2 ]
```

Ez a `'h'` tokenhez tartozik (`xs[0] = 3`), a c√©l: `'e'` (`ys[0] = 2`)

1. **Softmax** kisz√°m√≠t√°sa (egyszer≈±s√≠tett):

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

P√©lda:

* $e^{0.1} ‚âà 1.105$
* $e^{0.9} ‚âà 2.459$
* √ñsszeg pl. $\sum_j e^{z_j} ‚âà 7.0$

A c√©l logit a 2. index≈±:

* $p_{\text{c√©l}} = \frac{e^{0.9}}{7.0} ‚âà \frac{2.459}{7.0} ‚âà 0.351$

2. **Cross-entropy loss** erre a sorra:

$$
-\log(0.351) ‚âà 1.046
$$

Ez megmutatja, hogy a modell mennyire volt "bizonytalan" a helyes v√°lasszal kapcsolatban ‚Äî ha 1.0 alatt van a loss, m√°r eg√©sz j√≥.

---

## üß† Mi t√∂rt√©nik t√∂bb sor eset√©n?

Ugyanezt v√©gzi el minden soron:

```python
loss = mean([-log(p1), -log(p2), -log(p3)])
```

Ahol `p1`, `p2`, `p3` a helyes karakterek val√≥sz√≠n≈±s√©gei a softmax ut√°n.

---

## üìå Mi√©rt j√≥ a `cross_entropy`?

Mert:

* **oszt√°lyoz√°si feladathoz** val√≥ (nem sz√°mok √∂sszehasonl√≠t√°s√°hoz, hanem kateg√≥ri√°khoz),
* **softmax + log loss** egy√ºtt k√©nyelmesen kisz√°m√≠that√≥ (√©s numerikusan stabil is),
* **er≈ësen b√ºnteti a rossz biztos v√°laszokat**: ha a modell nagyon ‚Äûmagabiztos‚Äù, de t√©ved, nagy lesz a vesztes√©g.




## üéØ P√©lda: `logits = [1.0, 2.0, 3.0]`, c√©l: `2. oszt√°ly` (index = 2)

### 1. **Softmax sz√°m√≠t√°s**

El≈ësz√∂r a logitokb√≥l *val√≥sz√≠n≈±s√©geket* sz√°molunk:

$$
\text{softmax}_i = \frac{e^{\text{logit}_i}}{\sum_j e^{\text{logit}_j}}
$$

$$
\begin{align*}
e^1   &= 2.718 \\
e^2   &= 7.389 \\
e^3   &= 20.086 \\
\text{√ñsszeg} &= 2.718 + 7.389 + 20.086 = 30.193
\end{align*}
$$

Teh√°t:

$$
\begin{align*}
\text{softmax}_0 &= \frac{2.718}{30.193} ‚âà 0.09 \\
\text{softmax}_1 &= \frac{7.389}{30.193} ‚âà 0.24 \\
\text{softmax}_2 &= \frac{20.086}{30.193} ‚âà 0.66
\end{align*}
$$

---

### 2. **Cross-entropy loss**

A c√©loszt√°ly a 2-es (az utols√≥). Az ≈ë val√≥sz√≠n≈±s√©ge: `p = 0.66`

A `cross_entropy`:

$$
\text{loss} = -\log(p) = -\log(0.66) ‚âà 0.415
$$

Ez egy viszonylag **alacsony hiba**, mert a modell **j√≥ oszt√°lyt j√≥solt, √©s el√©g magabiztosan** (66%).

---

### üîé Mi t√∂rt√©nne, ha rossz oszt√°lyt j√≥solna?

Tegy√ºk fel, hogy a softmax:

```python
[0.66, 0.24, 0.09]  # helyes oszt√°ly most a 2., de a legkisebb p!
```

A loss ekkor:

$$
-\log(0.09) ‚âà 2.41
$$

üëâ **Nagy hiba** ‚Äì a modell rossz v√°laszt adott, r√°ad√°sul **nagyon magabiztosan** m√°sikban hitt.

---


## P√©lda
---

## üî° Bemenet √©s c√©l

* `xs = "hel"` ‚Üí tokenek: `[3, 2, 4]` ‚Üí `'h'`, `'e'`, `'l'`
* `ys = "elo"` ‚Üí tokenek: `[2, 4, 5]` ‚Üí `'e'`, `'l'`, `'o'`

## üî¢ Logit-sorok a `W` m√°trixb√≥l

* `W[3] = [0.1, 0.0, 0.9, 0.0, 0.2, 0.0, 0.0, 0.0]` ‚Üí `'h'` ‚Üí c√©l: `'e'` (index 2)
* `W[2] = [0.3, 0.0, 0.1, 0.1, 1.0, 0.0, 0.0, 0.0]` ‚Üí `'e'` ‚Üí c√©l: `'l'` (index 4)
* `W[4] = [-0.2, 0.1, 0.0, 0.0, 0.1, 0.9, 0.0, 0.0]` ‚Üí `'l'` ‚Üí c√©l: `'o'` (index 5)

## üìà Softmax ‚Üí c√©lkarakter val√≥sz√≠n≈±s√©gek

* `'h' ‚Üí 'e'`:
  softmax √©rt√©k ‚âà **0.449** ‚Üí `loss = -log(0.449) ‚âà 0.80`
* `'e' ‚Üí 'l'`:
  softmax √©rt√©k ‚âà **0.506** ‚Üí `loss = -log(0.506) ‚âà 0.68`
* `'l' ‚Üí 'o'`:
  softmax √©rt√©k ‚âà **0.449** ‚Üí `loss = -log(0.449) ‚âà 0.80`

## üßÆ √Åtlagolt `cross-entropy` vesztes√©g

$$
\text{√°tlagos loss} ‚âà (0.80 + 0.68 + 0.80) / 3 ‚âà 0.76
$$

---

Ez a loss √©rt√©k azt mutatja, hogy a modell **m√°r el√©g j√≥**, de **m√©g nem biztos** teljesen a v√°lasz√°ban (kb. 45‚Äì50%-os val√≥sz√≠n≈±s√©ggel j√≥solja a c√©l karaktereket).
