 **meg√©p√≠tj√ºk a legegyszer≈±bb nyelvi modellt**, ami karakterekb≈ël pr√≥b√°lja megtanulni a k√∂vetkez≈ë karaktert **mindenf√©le Transformer vagy Attention n√©lk√ºl**.

---

## üéØ C√©l: Els≈ë egyszer≈± nyelvi modell

Olyan modellt √≠runk, ami:

* bemenetk√©nt egy karakterl√°ncot (pl. `"hel"`) kap,
* √©s megpr√≥b√°lja megtippelni a k√∂vetkez≈ë karaktert (pl. `'l'` ‚Üí `'o'`).

Ehhez csak k√©t r√©teget haszn√°lunk:

1. `Embedding` ‚Äì a karaktereket vektorr√° alak√≠tja.
2. `Linear` ‚Äì megj√≥solja a k√∂vetkez≈ë karakter logitjait.

---

Nagyon j√≥, hogy ennyire alaposan akarod meg√©rteni ‚Äî ez a legjobb m√≥dja annak, hogy ne csak haszn√°ld a neur√°lis h√°l√≥kat, hanem **igaz√°n √©rtsd is, hogyan m≈±k√∂dnek bel√ºlr≈ël**. Kezdj√ºk t√©nyleg **null√°r√≥l**, √©s l√©p√©sr≈ël l√©p√©sre bontsuk ki:

---

# üî• 1. Mire val√≥ a `torch` k√∂nyvt√°r?

A `torch`, vagyis **PyTorch**, egy **ny√≠lt forr√°sk√≥d√∫ g√©pi tanul√°si k√∂nyvt√°r**, amit a Meta (Facebook) fejleszt. Arra val√≥, hogy:

| Feladat                      | Mire haszn√°lhat√≥?                        |
| ---------------------------- | ---------------------------------------- |
| ‚öôÔ∏è M√°trixm≈±veletek           | Olyan, mint a `numpy`, de GPU-n is fut   |
| üß† Neur√°lis h√°l√≥k √©p√≠t√©se    | R√©tegek, aktiv√°ci√≥k, vesztes√©gf√ºggv√©nyek |
| üîÅ Tan√≠t√°s (backpropagation) | Automatikus gradienssz√°m√≠t√°s             |
| üß™ Modell tan√≠t√°sa           | Optimizer, training loop, loss sz√°m√≠t√°s  |
| üì¶ Modell ment√©s/bet√∂lt√©s    | Tr√©ning ut√°n elmented a s√∫lyokat         |

Gyakorlatilag: **ez az az eszk√∂z, amivel saj√°t neur√°lis h√°l√≥kat tudsz √≠rni √©s edzeni.**

---

# üß± 2. Mi van a `torch.nn` modulban?

A `torch.nn` az **idegh√°l√≥zat-√©p√≠t≈ë modul** (nn = neural network). Tartalmaz mindent, amib≈ël h√°l√≥t tudsz √©p√≠teni:

| Tartalom            | P√©lda                                               |
| ------------------- | --------------------------------------------------- |
| R√©tegek             | `nn.Linear`, `nn.Embedding`, `nn.Conv2d`, `nn.LSTM` |
| Aktiv√°ci√≥k          | `nn.ReLU`, `nn.Sigmoid`, `nn.Tanh`                  |
| Norm√°l√°sok          | `nn.LayerNorm`, `nn.BatchNorm1d`                    |
| Vesztes√©gf√ºggv√©nyek | `nn.CrossEntropyLoss`, `nn.MSELoss`                 |
| Komplex egys√©gek    | `nn.Transformer`, `nn.Sequential`                   |

Ezeket haszn√°lod √∫gy, mint LEGO-kock√°kat: **√∂sszerakod ≈ëket egy nagy modell√©**.

---

# üß¨ 3. Mi az a `nn.Module`, √©s mi√©rt √∂r√∂kl√ºnk bel≈ële?

A `nn.Module` az **√∂sszes h√°l√≥zat alaposzt√°lya**. Ez tartalmazza:

| Funkci√≥             | Mi√©rt fontos?                                      |
| ------------------- | -------------------------------------------------- |
| `__init__()`        | Ide rakod a r√©tegeidet                             |
| `forward()`         | Itt hat√°rozod meg, hogyan halad √°t az adat a h√°l√≥n |
| `parameters()`      | Automatikusan tudja, mik a tan√≠that√≥ s√∫lyok        |
| `to(device)`        | √Åthelyez√©s CPU ‚Üî GPU k√∂z√∂tt                        |
| `eval()`, `train()` | V√°lt√°s edz≈ë / ki√©rt√©kel≈ë m√≥d k√∂z√∂tt                |
| `state_dict()`      | Ment√©shez, bet√∂lt√©shez sz√ºks√©ges bels≈ë √°llapot     |

### üîç Mi t√∂rt√©nne, ha nem √∂r√∂k√∂ln√©nk?

Akkor:

* nem lenne egys√©ges strukt√∫r√°ja a modellednek,
* nem tudn√°d `optimizer.step()`-pel friss√≠teni a s√∫lyokat,
* nem lehetne `model(x)` m√≥don megh√≠vni a h√°l√≥t,
* nem tudn√°d lementeni `.pt` f√°jlba.

Teh√°t a `nn.Module` az **a "keret", ami k√∂r√© a tanulhat√≥, menthet≈ë √©s haszn√°lhat√≥ neur√°lis h√°l√≥ √©p√ºl**.

---

## üõ†Ô∏è Mi t√∂rt√©nik val√≥j√°ban?

Amikor ezt √≠rod:

```python
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = nn.Linear(10, 20)

    def forward(self, x):
        return self.layer(x)
```

akkor a PyTorch:

* automatikusan elmenti a `self.layer` param√©tereit,
* ismeri a `.forward()` utas√≠t√°saidat,
* tudja majd, hogy ezt egy teljes modellk√©nt kell kezelni,
* √©s integr√°lja az **optimiz√°l√°s**, **ment√©s**, **inferencia** rendszer√©be.

---

# ‚úÖ √ñsszefoglal√°s

| Fogalom                  | Mire val√≥?                                                                                |
| ------------------------ | ----------------------------------------------------------------------------------------- |
| `torch`                  | G√©pi tanul√°si alap ‚Äì sz√°m√≠t√°s, tensor, GPU                                                |
| `torch.nn`               | R√©tegek, aktiv√°ci√≥k, vesztes√©gek, modellek                                                |
| `nn.Module`              | A modell ‚Äûv√°za‚Äù ‚Äì √∂sszefogja a r√©tegeket √©s tud tanulni                                   |
| Mi√©rt nem √≠runk saj√°tot? | Mert a `nn.Module` m√°r tud mindent, amire sz√ºks√©g van (s√∫lykezel√©s, tan√≠t√°s, ment√©s stb.) |

---

Szeretn√©d, hogy most mutassak egy konkr√©t p√©ld√°t arra, hogyan m≈±k√∂dik egy teljesen minim√°lis `nn.Module` oszt√°ly, √©s hogyan t√∂rt√©nik benne el≈ësz√∂r egy ‚Äûel≈ërehalad√°s‚Äù (forward pass)?




## üíª K√≥d: `bigram_model.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BigramModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        # Ez lesz az embedding √©s egyben a kimenet is
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def forward(self, idx, targets=None):
        # idx: (B, T) ‚Äì batch, time
        logits = self.token_embedding_table(idx)  # (B, T, vocab_size)

        if targets is None:
            loss = None
        else:
            # √Åtalak√≠tjuk a bemeneteket lapos t√∂mbb√© (batch*T), hogy loss-t sz√°moljunk
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        for _ in range(max_new_tokens):
            logits, _ = self(idx)
            logits = logits[:, -1, :]  # csak az utols√≥ tokenre
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, next_token), dim=1)
        return idx
```

---

## üì¶ Mi t√∂rt√©nik itt?

| R√©sz                                   | Mit csin√°l?                                                   |
| -------------------------------------- | ------------------------------------------------------------- |
| `nn.Embedding(vocab_size, vocab_size)` | Minden tokennek egy saj√°t ‚Äûj√≥sl√≥‚Äù vektora van                 |
| `forward()`                            | Visszaadja a logitokat, √©s ha van `targets`, loss-t is sz√°mol |
| `generate()`                           | Karaktereket gener√°l a modellb≈ël                              |

Ez m√©g **nem ‚Äûtanul‚Äù, csak defini√°lja a h√°l√≥t**.

---

## üìÅ L√©p√©s 2: egyszer≈± tan√≠t√≥ szkript

K√∂vetkez≈ë l√©p√©sben √≠runk hozz√° egy `train/train_bigram.py` f√°jlt, ami:

* bet√∂lti az `input.txt`-et,
* tokeniz√°l,
* minibatch-eken edzi a modellt,
* √©s n√©ha karaktereket gener√°l.

---

üëâ K√©szen √°llsz erre a k√∂vetkez≈ë l√©p√©sre? Vagy el≈ëbb szeretn√©d, hogy ezt a `BigramModel` oszt√°lyt r√©szletesen, soronk√©nt elmagyar√°zzam?
